{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based upon:\n",
    "# https://github.com/instadeepai/nucleotide-transformer/blob/main/examples/inference.ipynb\n",
    "\n",
    "# I installed nucleotide-transformer using: \n",
    "# pip install git+https://github.com/instadeepai/nucleotide-transformer@main\n",
    "\n",
    "# Another way to use nucleotide-transformer: \n",
    "# https://huggingface.co/InstaDeepAI/nucleotide-transformer-500m-human-ref?text=ACCTGA%3Cmask%3EAACTGAGTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from nucleotide_transformer.pretrained import get_pretrained_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model \n",
    "\n",
    "One must specify: \n",
    "\n",
    "1. the layers at which you'd like to collect embeddings (e.g. (5, 10, 20) to get embeddings at layers 5, 10 and 20)\n",
    "2. the attention maps you´d like to collect (e.g. ((1,4), (7,18)) to get attention maps corresponding to layer 1 head number 4 and layer 7 head number 18). Please refer to the config to see the number of layers and heads in the model.\n",
    "3. the maximum number of tokens in the sequences you'll compute the inference on. You can put values up to value specified in the model's config (counting the class token that will be added automatically at the beginning of the sequence), however we recommend keeping this number as small as possible for optimized memory and inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "model_name = '500M_human_ref' #@param['500M_human_ref', '500M_1000G', '2B5_1000G', '2B5_multi_species']\n",
    "\n",
    "SEQUENCE_LENGTH = 5977 # must be odd\n",
    "NUMBER_TOKENS_PER_SEQUENCE = 1000 # split sequence into book-ended 6-mer tokens plus CLS token\n",
    "\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=model_name,\n",
    "    mixed_precision=False,\n",
    "    embeddings_layers_to_save=(20,),\n",
    "    attention_maps_to_save=((1, 4), (7, 18)),\n",
    "    max_positions=NUMBER_TOKENS_PER_SEQUENCE \n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert NUMBER_TOKENS_PER_SEQUENCE <= config.max_positions # L91 of $HOME/.conda/envs/constraint-tools/lib/python3.9/site-packages/nucleotide_transformer/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# for inference: \n",
    "random_key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TODO] Plot variant-effect prediction profile and observed SNV positions for windows with varying degrees of predicted constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSTRAINT_TOOLS = '/scratch/ucgd/lustre-work/quinlan/u6018199/constraint-tools'\n",
    "CONSTRAINT_TOOLS_DATA = '/scratch/ucgd/lustre-work/quinlan/data-shared/constraint-tools'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chromosome</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>N_observed</th>\n",
       "      <th>N_bar_3_noncoding</th>\n",
       "      <th>N_mean_null_3_noncoding</th>\n",
       "      <th>N_variance_null_3_noncoding</th>\n",
       "      <th>cpg_island overlap</th>\n",
       "      <th>enhancer overlap</th>\n",
       "      <th>merged_exon overlap</th>\n",
       "      <th>window overlaps enhancer</th>\n",
       "      <th>window overlaps merged_exon</th>\n",
       "      <th>window overlaps cpg_island</th>\n",
       "      <th>new chen zscore</th>\n",
       "      <th>negative new chen zscore</th>\n",
       "      <th>eliteness_of_enhancer max</th>\n",
       "      <th>enhancer count</th>\n",
       "      <th>window_enhancer_overlap_bps sum</th>\n",
       "      <th>optimal_gene_targeted_by_enhancer &lt;lambda&gt;</th>\n",
       "      <th>optimal_enhancer_gene_association_score max</th>\n",
       "      <th>eliteness_of_optimal_enhancer_gene_association max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1432000</td>\n",
       "      <td>1433000</td>\n",
       "      <td>260</td>\n",
       "      <td>-0.247615</td>\n",
       "      <td>263.528599</td>\n",
       "      <td>203.072610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.299894</td>\n",
       "      <td>-4.299894</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>VWA1</td>\n",
       "      <td>758.12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1451000</td>\n",
       "      <td>1452000</td>\n",
       "      <td>293</td>\n",
       "      <td>2.367303</td>\n",
       "      <td>260.131776</td>\n",
       "      <td>192.772432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.666316</td>\n",
       "      <td>-0.666316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1453000</td>\n",
       "      <td>1454000</td>\n",
       "      <td>260</td>\n",
       "      <td>1.023579</td>\n",
       "      <td>245.832671</td>\n",
       "      <td>191.572571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.828398</td>\n",
       "      <td>-0.828398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1458000</td>\n",
       "      <td>1459000</td>\n",
       "      <td>274</td>\n",
       "      <td>1.921190</td>\n",
       "      <td>247.467287</td>\n",
       "      <td>190.731630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.086128</td>\n",
       "      <td>0.086128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1463000</td>\n",
       "      <td>1464000</td>\n",
       "      <td>192</td>\n",
       "      <td>-2.085869</td>\n",
       "      <td>220.639685</td>\n",
       "      <td>188.522242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.948188</td>\n",
       "      <td>-2.948188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>ATAD3C</td>\n",
       "      <td>750.33</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785813</th>\n",
       "      <td>chr9</td>\n",
       "      <td>137262000</td>\n",
       "      <td>137263000</td>\n",
       "      <td>303</td>\n",
       "      <td>4.256927</td>\n",
       "      <td>244.570154</td>\n",
       "      <td>188.398345</td>\n",
       "      <td>45.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-2.396149</td>\n",
       "      <td>2.396149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>346</td>\n",
       "      <td>DPH7,NELFB</td>\n",
       "      <td>23.85</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785814</th>\n",
       "      <td>chr9</td>\n",
       "      <td>137268000</td>\n",
       "      <td>137269000</td>\n",
       "      <td>215</td>\n",
       "      <td>-1.149755</td>\n",
       "      <td>230.788702</td>\n",
       "      <td>188.574204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.640544</td>\n",
       "      <td>-3.640544</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>HSALNG0075385</td>\n",
       "      <td>750.64</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785815</th>\n",
       "      <td>chr9</td>\n",
       "      <td>137269000</td>\n",
       "      <td>137270000</td>\n",
       "      <td>210</td>\n",
       "      <td>-3.445370</td>\n",
       "      <td>257.657992</td>\n",
       "      <td>191.337298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.276351</td>\n",
       "      <td>-5.276351</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>HSALNG0075385</td>\n",
       "      <td>750.64</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785816</th>\n",
       "      <td>chr9</td>\n",
       "      <td>137275000</td>\n",
       "      <td>137276000</td>\n",
       "      <td>296</td>\n",
       "      <td>1.921453</td>\n",
       "      <td>268.932391</td>\n",
       "      <td>198.445131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>712.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.687348</td>\n",
       "      <td>-2.687348</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>712</td>\n",
       "      <td>NELFB,EHMT1</td>\n",
       "      <td>762.48</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785818</th>\n",
       "      <td>chr9</td>\n",
       "      <td>137290000</td>\n",
       "      <td>137291000</td>\n",
       "      <td>224</td>\n",
       "      <td>-0.682673</td>\n",
       "      <td>233.338243</td>\n",
       "      <td>187.113535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.162811</td>\n",
       "      <td>-2.162811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1528560 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        chromosome      start        end  N_observed  N_bar_3_noncoding  \\\n",
       "0             chr1    1432000    1433000         260          -0.247615   \n",
       "4             chr1    1451000    1452000         293           2.367303   \n",
       "5             chr1    1453000    1454000         260           1.023579   \n",
       "7             chr1    1458000    1459000         274           1.921190   \n",
       "11            chr1    1463000    1464000         192          -2.085869   \n",
       "...            ...        ...        ...         ...                ...   \n",
       "1785813       chr9  137262000  137263000         303           4.256927   \n",
       "1785814       chr9  137268000  137269000         215          -1.149755   \n",
       "1785815       chr9  137269000  137270000         210          -3.445370   \n",
       "1785816       chr9  137275000  137276000         296           1.921453   \n",
       "1785818       chr9  137290000  137291000         224          -0.682673   \n",
       "\n",
       "         N_mean_null_3_noncoding  N_variance_null_3_noncoding  \\\n",
       "0                     263.528599                   203.072610   \n",
       "4                     260.131776                   192.772432   \n",
       "5                     245.832671                   191.572571   \n",
       "7                     247.467287                   190.731630   \n",
       "11                    220.639685                   188.522242   \n",
       "...                          ...                          ...   \n",
       "1785813               244.570154                   188.398345   \n",
       "1785814               230.788702                   188.574204   \n",
       "1785815               257.657992                   191.337298   \n",
       "1785816               268.932391                   198.445131   \n",
       "1785818               233.338243                   187.113535   \n",
       "\n",
       "         cpg_island overlap  enhancer overlap  merged_exon overlap  \\\n",
       "0                       NaN            1000.0                  NaN   \n",
       "4                       NaN               NaN                  NaN   \n",
       "5                       NaN               NaN                  NaN   \n",
       "7                       NaN               NaN                  NaN   \n",
       "11                      NaN              28.0                  NaN   \n",
       "...                     ...               ...                  ...   \n",
       "1785813                45.0             346.0                  NaN   \n",
       "1785814                 NaN            1000.0                  NaN   \n",
       "1785815                 NaN            1000.0                  NaN   \n",
       "1785816                 NaN             712.0                  NaN   \n",
       "1785818                 NaN               NaN                  NaN   \n",
       "\n",
       "         window overlaps enhancer  window overlaps merged_exon  \\\n",
       "0                            True                        False   \n",
       "4                           False                        False   \n",
       "5                           False                        False   \n",
       "7                           False                        False   \n",
       "11                           True                        False   \n",
       "...                           ...                          ...   \n",
       "1785813                      True                        False   \n",
       "1785814                      True                        False   \n",
       "1785815                      True                        False   \n",
       "1785816                      True                        False   \n",
       "1785818                     False                        False   \n",
       "\n",
       "         window overlaps cpg_island  new chen zscore  \\\n",
       "0                             False         4.299894   \n",
       "4                             False         0.666316   \n",
       "5                             False         0.828398   \n",
       "7                             False        -0.086128   \n",
       "11                            False         2.948188   \n",
       "...                             ...              ...   \n",
       "1785813                        True        -2.396149   \n",
       "1785814                       False         3.640544   \n",
       "1785815                       False         5.276351   \n",
       "1785816                       False         2.687348   \n",
       "1785818                       False         2.162811   \n",
       "\n",
       "         negative new chen zscore  eliteness_of_enhancer max  enhancer count  \\\n",
       "0                       -4.299894                        1.0               1   \n",
       "4                       -0.666316                        NaN               0   \n",
       "5                       -0.828398                        NaN               0   \n",
       "7                        0.086128                        NaN               0   \n",
       "11                      -2.948188                        1.0               1   \n",
       "...                           ...                        ...             ...   \n",
       "1785813                  2.396149                        0.0               2   \n",
       "1785814                 -3.640544                        1.0               1   \n",
       "1785815                 -5.276351                        1.0               1   \n",
       "1785816                 -2.687348                        1.0               2   \n",
       "1785818                 -2.162811                        NaN               0   \n",
       "\n",
       "         window_enhancer_overlap_bps sum  \\\n",
       "0                                   1000   \n",
       "4                                      0   \n",
       "5                                      0   \n",
       "7                                      0   \n",
       "11                                    28   \n",
       "...                                  ...   \n",
       "1785813                              346   \n",
       "1785814                             1000   \n",
       "1785815                             1000   \n",
       "1785816                              712   \n",
       "1785818                                0   \n",
       "\n",
       "        optimal_gene_targeted_by_enhancer <lambda>  \\\n",
       "0                                             VWA1   \n",
       "4                                                .   \n",
       "5                                                .   \n",
       "7                                                .   \n",
       "11                                          ATAD3C   \n",
       "...                                            ...   \n",
       "1785813                                 DPH7,NELFB   \n",
       "1785814                              HSALNG0075385   \n",
       "1785815                              HSALNG0075385   \n",
       "1785816                                NELFB,EHMT1   \n",
       "1785818                                          .   \n",
       "\n",
       "         optimal_enhancer_gene_association_score max  \\\n",
       "0                                             758.12   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "7                                                NaN   \n",
       "11                                            750.33   \n",
       "...                                              ...   \n",
       "1785813                                        23.85   \n",
       "1785814                                       750.64   \n",
       "1785815                                       750.64   \n",
       "1785816                                       762.48   \n",
       "1785818                                          NaN   \n",
       "\n",
       "         eliteness_of_optimal_enhancer_gene_association max  \n",
       "0                                                      1.0   \n",
       "4                                                      NaN   \n",
       "5                                                      NaN   \n",
       "7                                                      NaN   \n",
       "11                                                     1.0   \n",
       "...                                                    ...   \n",
       "1785813                                                1.0   \n",
       "1785814                                                1.0   \n",
       "1785815                                                1.0   \n",
       "1785816                                                1.0   \n",
       "1785818                                                NaN   \n",
       "\n",
       "[1528560 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "def get_windows_scores_annotations():\n",
    "    filename = f'{CONSTRAINT_TOOLS_DATA}/benchmark-genome-wide-predictions/chen-et-al-2022/enhancer-characteristics-enrichment.bed'\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    df = df[df['window overlaps merged_exon'] == False]\n",
    "    return df\n",
    "\n",
    "windows_scores_annotations_noncoding = get_windows_scores_annotations()\n",
    "windows_scores_annotations_noncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d array:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n",
      "raveled array:\n",
      "[1 2 3 4 5 6 7 8]\n",
      "<class 'numpy.ndarray'>\n",
      "unraveled array:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mSequence for region chr6:30214000-30214001:\u001b[0m\n",
      "\u001b[90ma\u001b[0m\n",
      "\u001b[36mIterating over region chr6:30214000-30214001 ...\u001b[0m\n",
      "\u001b[90mInterrogated 1/1 sites in region chr6:30214000-30214001\u001b[0m\n",
      "\u001b[90mNumber of sites in chr6:30214000-30214001 containing unspecified bases: 0/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1000)\n",
      "dict_keys(['attention_map_layer_1_number_4', 'attention_map_layer_7_number_18', 'embeddings_20', 'logits'])\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np \n",
    "from numpy import linalg as LA\n",
    "\n",
    "import sys\n",
    "sys.path.append(f'{CONSTRAINT_TOOLS}/utilities')\n",
    "\n",
    "import pysam \n",
    "\n",
    "from pack_unpack import pack \n",
    "from read_model import read_model\n",
    "from kmer import fetch_kmers, middle_index\n",
    "\n",
    "def compute_array_of_quantiles():\n",
    "  array_of_quantiles = [0.00, 0.005, 0.01, 0.02, 0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95, 0.98, 0.99, 0.995, 1.00]\n",
    "  # array_of_quantiles = [0.00, 0.01, 0.02, 0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95, 0.98, 0.99, 1.00]\n",
    "  starts = array_of_quantiles[:-1]\n",
    "  ends = array_of_quantiles[1:]\n",
    "  quantile_labels = [f'{start} - {end}' for start, end in zip(starts, ends)]\n",
    "  return array_of_quantiles, quantile_labels\n",
    "\n",
    "def label_windows_with_score_quantiles_core(df, score): \n",
    "  with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")        \n",
    "    array_of_quantiles, quantile_labels = compute_array_of_quantiles()\n",
    "    df[f'{score} quantile'], bins = pd.qcut(\n",
    "      df[score],\n",
    "      q = array_of_quantiles, \n",
    "      labels = quantile_labels, \n",
    "      retbins = True,\n",
    "#         duplicates='drop'\n",
    "    )\n",
    "    return df\n",
    "\n",
    "MCHALE_MODEL_FILENAME = f\"{CONSTRAINT_TOOLS}/dist/model-germline-grch38-Nonly.kmerSize-3.trainSet-noncoding.json\"\n",
    "MCHALE_MODEL = read_model(MCHALE_MODEL_FILENAME)\n",
    "\n",
    "BASES = 'ACGT'\n",
    "\n",
    "def set_center_nucleotide(sequence, new_center_nucleotide): \n",
    "  sequence = list(sequence)\n",
    "  sequence[middle_index(sequence)] = new_center_nucleotide\n",
    "  return ''.join(sequence)\n",
    "   \n",
    "def test_set_center_nucleotide(): \n",
    "   sequence = 'AAGCT'\n",
    "   new_center_nucleotide = 'T'\n",
    "   new_sequence = f'AA{new_center_nucleotide}CT'\n",
    "   assert set_center_nucleotide(sequence, new_center_nucleotide) == new_sequence\n",
    "\n",
    "test_set_center_nucleotide() \n",
    "\n",
    "def ravel(sequences):  \n",
    "  number_nucleotides, number_sites = sequences.shape\n",
    "  return np.reshape(sequences, number_nucleotides*number_sites, order='C')\n",
    "\n",
    "def unravel(sequences, number_nucleotides, number_sites): \n",
    "  return np.reshape(sequences, (number_nucleotides, number_sites), order='C')   \n",
    "\n",
    "def test_ravel_and_unravel(): \n",
    "  sequences = np.array([[1,2], [3,4], [5,6], [7,8]])\n",
    "  number_nucleotides, number_sites = sequences.shape\n",
    "  print('2d array:')\n",
    "  print(sequences)\n",
    "\n",
    "  sequences_raveled = ravel(sequences)\n",
    "  print('raveled array:')\n",
    "  print(sequences_raveled)\n",
    "  print(type(sequences_raveled))\n",
    "\n",
    "  sequences_unraveled = unravel(sequences_raveled, number_nucleotides, number_sites)\n",
    "  print('unraveled array:')\n",
    "  print(sequences_unraveled)\n",
    "\n",
    "  comparison = sequences_unraveled == sequences\n",
    "  assert comparison.all()\n",
    "\n",
    "test_ravel_and_unravel()\n",
    "\n",
    "def do_inference_on_window(window): \n",
    "  # TODO: re-instate following line once I've gotten 1 site coded up \n",
    "  # region = pack(window['chromosome'], window['start'], window['end'])\n",
    "  region = pack(window['chromosome'], window['start'], window['start'] + 1)\n",
    "  sequences = []\n",
    "  with pysam.FastaFile(MCHALE_MODEL['genome']) as genome:\n",
    "    for sequence in fetch_kmers(region, genome, kmer_size=SEQUENCE_LENGTH, log=True): # upper case\n",
    "      sequences_for_site = []\n",
    "      for new_center_nucleotide in BASES:\n",
    "        sequence = set_center_nucleotide(sequence, new_center_nucleotide)\n",
    "        sequences_for_site.append(sequence)\n",
    "      sequences.append(sequences_for_site)\n",
    "  sequences = np.array(sequences).T\n",
    "  sequences_raveled = ravel(sequences)\n",
    "\n",
    "  # Tokenize sequence(s)\n",
    "  # The `tokens_str` variable shows how sequence(s) have been split into tokens. \n",
    "  # The token list will be padded to size `max_positions`.\n",
    "  tokens_ids = [b[1] for b in tokenizer.batch_tokenize(list(sequences_raveled))] # batch_tokenize expects \"List[str]\"\n",
    "  # tokens_str = [b[0] for b in tokenizer.batch_tokenize(list(sequences_raveled))] # batch_tokenize expects \"List[str]\"\n",
    "  tokens = jnp.asarray(tokens_ids, dtype=jnp.int32)\n",
    "  print(tokens.shape)\n",
    "\n",
    "  # Perform inference\n",
    "  # The first time you query this cell, it will be slower than usual because of the computation graph compilation.        \n",
    "  outs = forward_fn.apply(parameters, random_key, tokens)  # one site takes 20secs on CPU => 1000 sites takes 5hrs on CPU \n",
    "\n",
    "  return outs \n",
    "\n",
    "def do_inference(df, score='negative new chen zscore'):\n",
    "  df = label_windows_with_score_quantiles_core(df, score)\n",
    "  df = (\n",
    "    df\n",
    "    .groupby([f'{score} quantile'], as_index=True)\n",
    "    .apply(lambda df_subset: df_subset.sample())\n",
    "    .reset_index(drop=True)\n",
    "  )\n",
    "  for _, window in df.iterrows():\n",
    "    return do_inference_on_window(window)\n",
    "  # TODO: save to disk\n",
    "\n",
    "do_inference(windows_scores_annotations_noncoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(f'{CONSTRAINT_TOOLS}/predict-constraint/germline-model')\n",
    "\n",
    "from expected_observed_counts import compute_SNV_positions_frequencies\n",
    "\n",
    "# TODO: \n",
    "def compute_heat_map(outs): \n",
    "  print(outs.keys())\n",
    "  return \n",
    "\n",
    "  # Retrieve embeddings\n",
    "  print(outs[\"embeddings_20\"].shape)\n",
    "\n",
    "  embeddings = outs[\"embeddings_20\"][:, 1:, :]  # removing CLS token\n",
    "  padding_mask = jnp.expand_dims(tokens[:, 1:] != tokenizer.pad_token_id, axis=-1)\n",
    "  masked_embeddings = embeddings * padding_mask  \n",
    "  sequences_lengths = jnp.sum(padding_mask, axis=1)\n",
    "  mean_embeddings = jnp.sum(masked_embeddings, axis=1) / sequences_lengths\n",
    "  print(mean_embeddings)\n",
    "\n",
    "  # TODO:\n",
    "  # 1. \"unravel\" mean_embeddings \n",
    "  # 2. figure out how to identify the REF embedding for each site \n",
    "  ref_embedding = mean_embeddings[0,:]\n",
    "  alt_embedding = mean_embeddings[1,:]\n",
    "\n",
    "  # compute distance between embeddings as a heat map \n",
    "  #    (i) the L1 distance (Manhattan), \n",
    "  #    (ii) the L2 distance (Euclidean), \n",
    "  #    (iii) the cosine similarity \n",
    "  #    (iv) the dot-product (not normalized cosine similarity) \n",
    "  delta = ref_embedding - alt_embedding\n",
    "  distance = LA.norm(delta, 2) \n",
    "\n",
    "# TODO: \n",
    "def get_snvs(region): \n",
    "  with pysam.TabixFile(MCHALE_MODEL['mutations']) as mutations, pysam.FastaFile(MCHALE_MODEL['genome']) as genome:\n",
    "    (\n",
    "      SNV_positions_frequencies_CpG_positive, \n",
    "      SNV_positions_frequencies_CpG_negative\n",
    "    ) = compute_SNV_positions_frequencies(mutations, genome, region, MCHALE_MODEL)\n",
    "  return (\n",
    "    SNV_positions_frequencies_CpG_positive, \n",
    "    SNV_positions_frequencies_CpG_negative,\n",
    "  )\n",
    "\n",
    "def plot_snvs_and_heat_map(): \n",
    "  # TODO: \n",
    "  # plot locations of observed SNVs (use web-browser plotly.js code???), (weighted by allele frequency)\n",
    "\n",
    "  # TODO: \n",
    "  # compute variant-effects (compute_heat_map) \n",
    "\n",
    "  # plot variant-effect 4*L heat map\n",
    "  #     c.f., experiments/germline-model/chen-et-al-2022/interplay-of-constraint-and-sequence-feature-composition.1.ipynb\n",
    "  pass "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TODO] Hypothesis test\n",
    "outlined in 2a: https://docs.google.com/presentation/d/1naB2zEBWXLoQOP9ioMDe77FlP8dhb63Eu8P_G6Jdpac/edit#slide=id.g22afc145c47_0_8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Other resources] Get attention maps\n",
    "Here is an example on how to retrieve attention maps at a specific layer for a given head and how to plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outs[\"attention_map_layer_1_number_4\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "sns.set_theme(\n",
    "    font_scale=2,\n",
    "    rc={'figure.figsize': (16, 6)}\n",
    ")\n",
    "\n",
    "# plot attention maps\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "seq_length0, seq_length1 = int(sequences_lengths[0]), int(sequences_lengths[1])\n",
    "\n",
    "# plot for first seq in the batch\n",
    "im0 = axes[0].imshow(\n",
    "    outs[\"attention_map_layer_1_number_4\"][\n",
    "        0, 1:(seq_length0 + 1), 1:(seq_length0 + 1)\n",
    "    ]\n",
    ")\n",
    "divider0 = make_axes_locatable(axes[0])\n",
    "cax0 = divider0.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "tokens0 = tokens_str[0][1 : (seq_length0 + 1)]\n",
    "axes[0].set_xticks(list(range(seq_length0)))\n",
    "axes[0].set_xticklabels(tokens0, rotation=45)\n",
    "axes[0].set_yticks(list(range(seq_length0)))\n",
    "axes[0].set_yticklabels(tokens0, rotation=45)\n",
    "axes[0].grid(False)\n",
    "fig.colorbar(im0, cax=cax0, orientation=\"vertical\")\n",
    "\n",
    "# plot for second seq in the batch\n",
    "im1 = axes[1].imshow(\n",
    "    outs[\"attention_map_layer_1_number_4\"][\n",
    "        1, 1:(seq_length1 + 1), 1:(seq_length1 + 1)\n",
    "    ]\n",
    ")\n",
    "divider1 = make_axes_locatable(axes[1])\n",
    "cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "tokens1 = tokens_str[1][1 : (seq_length1 + 1)]\n",
    "axes[1].set_xticks(list(range(seq_length1)))\n",
    "axes[1].set_xticklabels(tokens1, rotation=45)\n",
    "axes[1].set_yticks(list(range(seq_length1)))\n",
    "axes[1].set_yticklabels(tokens1, rotation=45)\n",
    "axes[1].grid(False)\n",
    "fig.colorbar(im1, cax=cax1, orientation=\"vertical\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Other resources] Get probabilities\n",
    "Finally, let's look at the model probabilities over the vocabulary at each position. These can be used to compute reconstruction accuracies and perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outs[\"logits\"]\n",
    "probabilities = []\n",
    "\n",
    "# get probabilities separately for each seq as they have different lengths\n",
    "for seq_id in range(logits.shape[0]):\n",
    "\n",
    "    logits_seq = logits[seq_id]\n",
    "    seq_length = int(sequences_lengths[seq_id])\n",
    "    logits_seq = logits_seq[1 : (seq_length + 1)]  # remove CLS token and pads\n",
    "    probas = jax.nn.softmax(\n",
    "        logits_seq, axis=-1\n",
    "    )  # use softmax to transform logits into probabilities\n",
    "\n",
    "    print(probas.shape)\n",
    "    probabilities.append(probas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look in particular at a given sequence and position and show the top-k probabilities and corresponding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_id = 0\n",
    "position_id = 1\n",
    "\n",
    "probs = probabilities[sequence_id][position_id]\n",
    "sorted_positions = jnp.argsort(-probs)\n",
    "sorted_probs = probs[sorted_positions]\n",
    "\n",
    "top_k = 5\n",
    "for k in range(top_k):\n",
    "    predicted_token = tokenizer.id_to_token(int(sorted_positions[k]))\n",
    "    prob = sorted_probs[k]\n",
    "    print(f\"token: {predicted_token}, probability: {prob * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "constraint-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
