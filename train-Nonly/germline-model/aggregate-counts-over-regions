#!/usr/bin/env python

# pysam API: 
# https://github.com/pysam-developers/pysam/blob/b82cbcae22c088e64fdb58f8acaf1e9773c7b088/pysam/libctabix.pyx
import pysam

import json
import argparse 
from tqdm import tqdm

from kmer import initialize_kmer_counts_germline
from kmer_counts import compute_kmer_total_counts, compute_kmer_SNV_counts
from aggregate_counts import aggregate_counts

from colorize import (
  print_json, 
  print_string_as_info, 
  print_string_as_info_dim
)
import color_traceback 
from pack_unpack import bed_to_sam_string, unpack
from timer import timer 
from hostname_process_cpu import get_hostname_process_cpu

def parse_arguments():
  parser = argparse.ArgumentParser(description='')
  parser.add_argument('--kmer-size', type=int, dest='kmer_size', help='')
  parser.add_argument('--genome', type=str, help='')
  parser.add_argument('--number-chromosomes-min', type=int, dest='number_chromosomes_min', help='')
  parser.add_argument('--mutations', type=str, help='')
  parser.add_argument('--train-regions-filename', type=str, dest='train_regions_filename', help='')
  parser.add_argument('--counts-filename', type=str, dest='counts_filename', help='')
  parser.add_argument('--log', type=str, help='')
  return parser.parse_args()

@timer
def count_on_region(train_region, args):
  # pysam.FastaFile uses the index produced by "samtools faidx": 
  # https://pysam.readthedocs.io/en/latest/api.html?highlight=fasta#pysam.FastaFile
  with pysam.TabixFile(args.mutations) as mutations, pysam.FastaFile(args.genome) as genome: 
    kmer_counts = initialize_kmer_counts_germline(args) 
    kmer_counts = compute_kmer_total_counts(train_region, genome, kmer_counts, args) 
    kmer_counts = compute_kmer_SNV_counts(train_region, mutations, genome, kmer_counts, args)

  return kmer_counts

# https://realpython.com/introduction-to-python-generators/
def fetch_counts_from_regions(args, progress_bar): 
  print_json({'train regions': args.train_regions_filename, **get_hostname_process_cpu()})
  with open(args.train_regions_filename, 'r') as train_regions:
    number_of_train_regions = sum(1 for line in train_regions)
  with open(args.train_regions_filename, 'r') as train_regions:
    for train_region in tqdm(
      train_regions, 
      total=number_of_train_regions,
      file=progress_bar, 
      desc='fetch_counts_from_regions'
    ): 
      train_region = bed_to_sam_string(train_region)
      kmer_counts = count_on_region(train_region, args)      
      yield { 
        'kmerCounts': kmer_counts,
      }

def aggregate_counts_over_regions(args, progress_bar): 
  # Minimize high-latency IO by ONLY writing counts AFTER aggregation over regions.
  # A similar approach is used by spark to get 100X speed-ups relative to Hadoop:
  # https://www.coursera.org/learn/scala-spark-big-data/lecture/D5o7O/latency

  print_string_as_info('Aggregating kmer counts over regions...')
  counts = aggregate_counts(fetch_counts_from_regions, args, progress_bar)

  print_string_as_info(f'Aggregated kmer counts over regions:') 
  print_string_as_info_dim(args.train_regions_filename) 
  print_string_as_info('Saved aggregated counts to:')
  print_string_as_info_dim(args.counts_filename)
  
  with open(args.counts_filename, 'w') as fh:
    json.dump({
      'mutations': args.mutations,
      'genome': args.genome,
      'kmerSize': args.kmer_size,
      'numberChromosomesMin': args.number_chromosomes_min,
      'kmerCounts': counts['kmerCounts'],
    }, fh, indent=2)

def main(): 
  args = parse_arguments()  
  import sys 
  if args.log == 'stdout': 
    aggregate_counts_over_regions(args, progress_bar=sys.stdout)
  else: 
    with open(args.log, 'w') as progress_bar:  
      # logging tqdm output: 
      # https://github.com/tqdm/tqdm/issues/506#issuecomment-373762049
      # https://github.com/tqdm/tqdm/issues/506#issuecomment-508458426
      aggregate_counts_over_regions(args, progress_bar=progress_bar)

if __name__ == '__main__':
  main() 

