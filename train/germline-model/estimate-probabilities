#!/usr/bin/env python

import os 
import argparse 
import json 
import numpy as np 
from tqdm import tqdm
import sys

from kmer import (
  compute_possible_ALT_states, 
  compute_kmers, 
)
from colorize import (
  print_string_as_info, 
  print_string_as_info_dim, 
  print_unbuffered
)
import color_traceback
from aggregate_counts import aggregate_counts 
from singleton import make_serializable, dict_to_defaultdict

def parse_arguments():
  parser = argparse.ArgumentParser(description='')
  parser.add_argument('--kmer-size', type=int, dest='kmer_size', help='')
  parser.add_argument('--genome', type=str, help='')
  parser.add_argument('--build', type=str, help='')
  parser.add_argument('--number-chromosomes-min', type=int, dest='number_chromosomes_min', help='')
  parser.add_argument('--tmpdir', type=str, help='')
  parser.add_argument('--mutations', type=str, help='')
  parser.add_argument('--window-size', type=int, dest='window_size', help='')
  parser.add_argument('--model', type=str, help='')
  parser.add_argument('--neutral-regions', type=str, dest='neutral_regions', help='')
  parser.add_argument('--progress-bars', type=str, dest='progress_bars', help='')
  parser.add_argument('--max-neutral-region-length', type=int, dest='max_neutral_region_length', help='')
  return parser.parse_args()

# https://realpython.com/introduction-to-python-generators/
def fetch_counts_from_jobs(args, progress_bar): 
  from glob import glob 
  for counts_filename in tqdm(glob(f'{args.tmpdir}/counts.*.json'), file=progress_bar, desc='fetch_counts_from_jobs'):
    with open(counts_filename, 'r') as fh:
      d = json.load(fh) 
      yield { 
        'kmerCounts': d['kmerCounts'],
        'singletonCounts': dict_to_defaultdict(d['singletonCounts'])
      }

def estimate_ALT_state_probability(kmer_counts, kmer, ALT_state): 
  data = kmer_counts[kmer]
  # estimate probabilities for multinomial distribution: https://math.stackexchange.com/a/421838
  return data['ALTStateCounts'][ALT_state]/data['count'] if data['count'] > 0 else None
  
def estimate_kmer_probabilities(kmer_counts, args):
  return { 
    kmer: { 
      ALT_state: estimate_ALT_state_probability(kmer_counts, kmer, ALT_state) 
      for ALT_state in compute_possible_ALT_states(kmer) 
    } 
    for kmer in compute_kmers(args.kmer_size)      
  }

def estimate_singleton_probabilities(singleton_counts): 
  return { 
    SNV_count: conditioned_singleton_counts/np.sum(conditioned_singleton_counts)
    for SNV_count, conditioned_singleton_counts in singleton_counts.items()
  } 

def estimate_probabilities(args, progress_bar, progress_bar_filename):
  print_string_as_info('Aggregating kmer and singleton counts over jobs using count files:')
  print_string_as_info_dim(f'{args.tmpdir}/counts.*.json')

  print_string_as_info('Logging progress of job aggregation to:')
  print_string_as_info_dim(progress_bar_filename)

  counts = aggregate_counts(fetch_counts_from_jobs, args, progress_bar)
  kmer_counts, singleton_counts = counts['kmerCounts'], counts['singletonCounts']

  print_string_as_info(
    'Estimating probability of observing ALT_state, e.g. {C, T}, given kmer, e.g., AAA, ...'
  )
  kmer_probabilities = estimate_kmer_probabilities(kmer_counts, args) 

  print_string_as_info(
    'Estimating the probability of observing k singletons, given that a window contains m SNVs...'
  )
  singleton_probabilities = estimate_singleton_probabilities(singleton_counts)

  print_string_as_info(f'Writing model to:')
  print_string_as_info_dim(f'{args.model}')
  with open(args.model, 'w') as fh:
    json.dump({
      'mutations': args.mutations,
      'genome': args.genome,
      'build': args.build,
      'neutralRegions': args.neutral_regions,
      'kmerSize': args.kmer_size,
      'numberChromosomesMin': args.number_chromosomes_min,
      'windowSize': args.window_size,
      'maxNeutralRegionLength': args.max_neutral_region_length,
      'kmerCounts': kmer_counts,
      'singletonCounts': make_serializable(singleton_counts),
      'kmerProbabilities': kmer_probabilities,
      'singletonProbabilities': make_serializable(singleton_probabilities)
    }, fh, indent=2)

def main(): 
  args = parse_arguments()  
  if args.progress_bars == 'stdout': 
    estimate_probabilities(args, sys.stdout, 'stdout')
  elif args.progress_bars == 'disk':
    progress_bar_filename = f'{args.tmpdir}/progress-bar.log'
    with open(progress_bar_filename, 'w') as progress_bar:
      estimate_probabilities(args, progress_bar, progress_bar_filename)
  else: 
    raise ValueError(f'invalid value for progress-bars: {args.progress_bars}')

if __name__ == '__main__': 
  main() 



