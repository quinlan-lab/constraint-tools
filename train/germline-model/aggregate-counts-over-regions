#!/usr/bin/env python

# pysam API: 
# https://github.com/pysam-developers/pysam/blob/b82cbcae22c088e64fdb58f8acaf1e9773c7b088/pysam/libctabix.pyx
import pysam

import numpy as np
import json
import argparse 
import os
import subprocess
import multiprocessing
from tqdm import tqdm

from kmer import (
  initialize_kmer_counts_germline, 
  fetch_kmer_from_sequence, 
  contains_unspecified_bases,
  middle_base
)
from singleton import Singleton_Counts, make_serializable
from aggregate_counts import aggregate_counts

from colorize import (
  print_json, 
  print_string_as_info, 
  print_string_as_info_dim, 
  print_unbuffered
)
import color_traceback 
from snvs import fetch_SNVs, reduce_SNVs
from pack_unpack import unpack, bed_to_sam_string
from timer import timer 
from windows import create_windows 

# using the "khmer" library might make this function a handful of times faster:
# https://khmer.readthedocs.io/en/v0.6.1-0/ktable.html
def compute_kmer_total_counts(neutral_region, genome, kmer_counts, args):
  # "fetch" API: https://pysam.readthedocs.io/en/latest/api.html?highlight=fasta#pysam.FastaFile
  # Note that fetch(region=region) does not work if the coordinates in "region" contains commas
  # Workaround is to parse "region" into "chromosome", "start", "end": 
  neutral_sequence = genome.fetch(*unpack(neutral_region))    
  print_string_as_info(f"Sequence for neutral region {neutral_region} is:")
  print_string_as_info_dim(f"{neutral_sequence}")
  print_unbuffered('')

  # TODO: refactor this to use the "windows" module 
  print_string_as_info('Iterating over neutral region {} and counting kmers:'.format(neutral_region))
  start_position = 0 + args.kmer_size//2
  end_position = len(neutral_sequence) - args.kmer_size//2 
  number_of_sites_containing_unspecified_bases = 0
  for position in np.arange(start_position, end_position, 1): 
    kmer = fetch_kmer_from_sequence(neutral_sequence, position, args.kmer_size)
    if contains_unspecified_bases(middle_base(kmer)): 
      number_of_sites_containing_unspecified_bases += 1
    if contains_unspecified_bases(kmer): continue 
    kmer_counts[kmer]['count'] += 1

  number_of_sites = end_position - start_position
  print_string_as_info_dim(
    f'Number of sites in {neutral_region} containing unspecified bases: '
    f'{number_of_sites_containing_unspecified_bases}/{number_of_sites}'
  )
  print_unbuffered('')

  return kmer_counts

def compute_kmer_SNV_counts(neutral_region, mutations, genome, kmer_counts, args):
  print_string_as_info('Fetching SNVs in region {} and incrementing corresponding kmer counts\n'.format(neutral_region))

  SNVs = fetch_SNVs(mutations, genome, neutral_region, args.__dict__)
  SNVs = reduce_SNVs(SNVs)
  for SNV in SNVs: 
    kmer_counts[SNV['kmer']]['ALTStateCounts'][SNV['ALTState']] += 1
  return kmer_counts

def get_hostname_process_cpu(): 
  hostname = os.uname()[1]
  pid = os.getpid()
  cpu = subprocess.run(
    ["ps", "-o", "psr=", "-p", f"{pid}"], 
    capture_output=True
  ).stdout.decode("utf-8").strip()
  return {
    'hostname': hostname,
    'process': pid, 
    'cpu': f'{cpu}/{multiprocessing.cpu_count()}'
  }

def parse_arguments():
  parser = argparse.ArgumentParser(description='')
  parser.add_argument('--kmer-size', type=int, dest='kmer_size', help='')
  parser.add_argument('--genome', type=str, help='')
  parser.add_argument('--number-chromosomes-min', type=int, dest='number_chromosomes_min', help='')
  parser.add_argument('--mutations', type=str, help='')
  parser.add_argument('--window-size', type=int, dest='window_size', help='')
  parser.add_argument('--neutral-regions-filename', type=str, dest='neutral_regions_filename', help='')
  parser.add_argument('--counts-filename', type=str, dest='counts_filename', help='')
  parser.add_argument('--log', type=str, help='')
  return parser.parse_args()

def compute_singleton_counts(neutral_region, mutations, genome, args):
  print_string_as_info('Breaking neutral region into non-overlapping windows, and counting SNVs and singletons in each window...')

  windows = create_windows(
    window_size=args.window_size, 
    window_stride=args.window_size, 
    region=neutral_region, 
    genome=genome, 
    region_contains_windows=True
  )
  print_string_as_info_dim(f'Created {len(windows)} windows within neutral region {neutral_region}')

  singleton_counts = Singleton_Counts()
  for window in windows: 
    SNVs = fetch_SNVs(mutations, genome, window['region'], args.__dict__, args.number_chromosomes_min)
    SNV_count = len(SNVs)
    singleton_count = len([SNV for SNV in SNVs if SNV['number_ALT_chromosomes'] == 1])
    print_string_as_info_dim(f"{singleton_count} of {SNV_count} SNVs in {window['region']} are singletons")
    singleton_counts[SNV_count][singleton_count] += 1

  print_unbuffered('')

  return singleton_counts

@timer
def count_on_region(neutral_region, args):
  # pysam.FastaFile uses the index produced by "samtools faidx": 
  # https://pysam.readthedocs.io/en/latest/api.html?highlight=fasta#pysam.FastaFile
  with pysam.TabixFile(args.mutations) as mutations, pysam.FastaFile(args.genome) as genome: 
    kmer_counts = initialize_kmer_counts_germline(args) 
    kmer_counts = compute_kmer_total_counts(neutral_region, genome, kmer_counts, args) 
    kmer_counts = compute_kmer_SNV_counts(neutral_region, mutations, genome, kmer_counts, args)

    singleton_counts = compute_singleton_counts(neutral_region, mutations, genome, args)

  return kmer_counts, singleton_counts

# https://realpython.com/introduction-to-python-generators/
def fetch_counts_from_regions(args, progress_bar): 
  print_json({'neutral regions': args.neutral_regions_filename, **get_hostname_process_cpu()})
  with open(args.neutral_regions_filename, 'r') as neutral_regions:
    for neutral_region in tqdm(neutral_regions, file=progress_bar, desc='fetch_counts_from_regions'): 
      neutral_region = bed_to_sam_string(neutral_region)
      kmer_counts, singleton_counts = count_on_region(neutral_region, args)      
      yield { 
        'kmerCounts': kmer_counts,
        'singletonCounts': singleton_counts
      }

def aggregate_counts_over_regions(args, progress_bar): 
  # Minimize high-latency IO by ONLY writing counts AFTER aggregation over regions.
  # A similar approach is used by spark to get 100X speed-ups relative to Hadoop:
  # https://www.coursera.org/learn/scala-spark-big-data/lecture/D5o7O/latency

  print_string_as_info('Aggregating kmer and singleton counts over regions...')
  counts = aggregate_counts(fetch_counts_from_regions, args, progress_bar)

  print_string_as_info(f'Aggregated kmer and singleton counts over regions:') 
  print_string_as_info_dim(args.neutral_regions_filename) 
  print_string_as_info('Saved aggregated counts to:')
  print_string_as_info_dim(args.counts_filename)
  
  with open(args.counts_filename, 'w') as fh:
    json.dump({
      'mutations': args.mutations,
      'genome': args.genome,
      'kmerSize': args.kmer_size,
      'numberChromosomesMin': args.number_chromosomes_min,
      'windowSize': args.window_size,
      'kmerCounts': counts['kmerCounts'],
      'singletonCounts': make_serializable(counts['singletonCounts']),
    }, fh, indent=2)

def main(): 
  args = parse_arguments()  
  import sys 
  if args.log == 'stdout': 
    aggregate_counts_over_regions(args, progress_bar=sys.stdout)
  else: 
    with open(args.log, 'w') as progress_bar:  
      # logging tqdm output: 
      # https://github.com/tqdm/tqdm/issues/506#issuecomment-373762049
      # https://github.com/tqdm/tqdm/issues/506#issuecomment-508458426
      aggregate_counts_over_regions(args, progress_bar=progress_bar)

if __name__ == '__main__':
  main()  

