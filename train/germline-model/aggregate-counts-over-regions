#!/usr/bin/env python

# pysam API: 
# https://github.com/pysam-developers/pysam/blob/b82cbcae22c088e64fdb58f8acaf1e9773c7b088/pysam/libctabix.pyx
import pysam

import json
import argparse 
from tqdm import tqdm

from kmer import initialize_kmer_counts_germline
from kmer_counts import compute_kmer_total_counts, compute_kmer_SNV_counts
from singleton import Singleton_Counts, make_serializable
from aggregate_counts import aggregate_counts

from colorize import (
  print_json, 
  print_string_as_info, 
  print_string_as_info_dim
)
import color_traceback 
from snvs import fetch_SNVs, reduce_SNVs
from pack_unpack import bed_to_sam_string, unpack
from timer import timer 
from windows import create_windows 
from get_M_K import get_M_K_trainingTime
from hostname_process_cpu import get_hostname_process_cpu

def parse_arguments():
  parser = argparse.ArgumentParser(description='')
  parser.add_argument('--kmer-size', type=int, dest='kmer_size', help='')
  parser.add_argument('--genome', type=str, help='')
  parser.add_argument('--number-chromosomes-min', type=int, dest='number_chromosomes_min', help='')
  parser.add_argument('--mutations', type=str, help='')
  parser.add_argument('--window-size', type=int, dest='window_size', help='')
  parser.add_argument('--trustworthy-noncoding-regions-filename', type=str, dest='trustworthy_noncoding_regions_filename', help='')
  parser.add_argument('--counts-filename', type=str, dest='counts_filename', help='')
  parser.add_argument('--log', type=str, help='')
  # https://docs.python.org/3/library/argparse.html#action:
  parser.add_argument('--test', action=argparse.BooleanOptionalAction)
  return parser.parse_args()

def compute_singleton_counts(trustworthy_noncoding_region, mutations, genome, args):
  print_string_as_info('Breaking trustworthy noncoding region into non-overlapping windows, and counting SNVs and singletons in each window...')

  windows = create_windows(
    window_size=args.window_size, 
    window_stride=args.window_size, 
    region=trustworthy_noncoding_region, 
    genome=genome, 
    region_contains_windows=True
  )
  print_string_as_info_dim(f'Created {len(windows)} windows within trustworthy noncoding region {trustworthy_noncoding_region}')

  singleton_counts = Singleton_Counts()
  for window in windows: 
    M, K = get_M_K_trainingTime(window, mutations, genome, args)
    singleton_counts[M][K] += 1

  return singleton_counts

@timer
def count_on_region(trustworthy_noncoding_region, args):
  # pysam.FastaFile uses the index produced by "samtools faidx": 
  # https://pysam.readthedocs.io/en/latest/api.html?highlight=fasta#pysam.FastaFile
  with pysam.TabixFile(args.mutations) as mutations, pysam.FastaFile(args.genome) as genome: 
    kmer_counts = initialize_kmer_counts_germline(args) 
    kmer_counts = compute_kmer_total_counts(trustworthy_noncoding_region, genome, kmer_counts, args) 
    kmer_counts = compute_kmer_SNV_counts(trustworthy_noncoding_region, mutations, genome, kmer_counts, args)

    singleton_counts = compute_singleton_counts(trustworthy_noncoding_region, mutations, genome, args)

  return kmer_counts, singleton_counts

# https://realpython.com/introduction-to-python-generators/
def fetch_counts_from_regions(args, progress_bar): 
  print_json({'trustworthy noncoding regions': args.trustworthy_noncoding_regions_filename, **get_hostname_process_cpu()})
  with open(args.trustworthy_noncoding_regions_filename, 'r') as trustworthy_noncoding_regions:
    number_of_trustworthy_noncoding_regions = sum(1 for line in trustworthy_noncoding_regions)
  with open(args.trustworthy_noncoding_regions_filename, 'r') as trustworthy_noncoding_regions:
    for trustworthy_noncoding_region in tqdm(
      trustworthy_noncoding_regions, 
      total=number_of_trustworthy_noncoding_regions,
      file=progress_bar, 
      desc='fetch_counts_from_regions'
    ): 
      trustworthy_noncoding_region = bed_to_sam_string(trustworthy_noncoding_region)
      kmer_counts, singleton_counts = count_on_region(trustworthy_noncoding_region, args)      
      yield { 
        'kmerCounts': kmer_counts,
        'singletonCounts': singleton_counts
      }

def aggregate_counts_over_regions(args, progress_bar): 
  # Minimize high-latency IO by ONLY writing counts AFTER aggregation over regions.
  # A similar approach is used by spark to get 100X speed-ups relative to Hadoop:
  # https://www.coursera.org/learn/scala-spark-big-data/lecture/D5o7O/latency

  print_string_as_info('Aggregating kmer and singleton counts over regions...')
  counts = aggregate_counts(fetch_counts_from_regions, args, progress_bar)

  print_string_as_info(f'Aggregated kmer and singleton counts over regions:') 
  print_string_as_info_dim(args.trustworthy_noncoding_regions_filename) 
  print_string_as_info('Saved aggregated counts to:')
  print_string_as_info_dim(args.counts_filename)
  
  with open(args.counts_filename, 'w') as fh:
    json.dump({
      'mutations': args.mutations,
      'genome': args.genome,
      'kmerSize': args.kmer_size,
      'numberChromosomesMin': args.number_chromosomes_min,
      'windowSize': args.window_size,
      'kmerCounts': counts['kmerCounts'],
      'singletonCounts': make_serializable(counts['singletonCounts']),
    }, fh, indent=2)

def main(): 
  args = parse_arguments()  
  import sys 
  if args.log == 'stdout': 
    aggregate_counts_over_regions(args, progress_bar=sys.stdout)
  else: 
    with open(args.log, 'w') as progress_bar:  
      # logging tqdm output: 
      # https://github.com/tqdm/tqdm/issues/506#issuecomment-373762049
      # https://github.com/tqdm/tqdm/issues/506#issuecomment-508458426
      aggregate_counts_over_regions(args, progress_bar=progress_bar)

  if args.test: 
    test_compute_kmer_total_counts(args) 
    test_compute_singleton_counts(args) 

def test_compute_kmer_total_counts(args): 
  print('')
  print('*************************************************')
  print('')
  print_string_as_info('**** testing compute_kmer_total_counts(...) ***** ')
  print('')

  region = 'chr1:15320-15340'
  region_with_flanks = 'chr1:15319-15341'

  with pysam.FastaFile(args.genome) as genome: 
    sequence_with_flanks = genome.fetch(*unpack(region_with_flanks))    
    print_string_as_info(f'sequence for region {region_with_flanks}')
    print_string_as_info_dim(sequence_with_flanks)

  with pysam.FastaFile(args.genome) as genome: 
    kmer_counts = initialize_kmer_counts_germline(args)
    expected_kmer_total_counts = [('AAA', 2), ('AAG', 2), ('AGA', 1), ('AGC', 3), ('AGG', 1), ('CAA', 2), ('CAG', 2), ('CGC', 1), ('GAG', 1), ('GCA', 3), ('GCG', 1), ('GGC', 1)]
    kmer_counts = compute_kmer_total_counts(region, genome, kmer_counts, args)
    observed_kmer_total_counts = [(kmer, data['count']) for kmer, data in kmer_counts.items() if data['count'] > 0]
    observed_number_kmers = sum([count for _, count in observed_kmer_total_counts])
    # assert observed_number_kmers == 20 - (args.kmer_size - 1) # 20 is the hard-coded sequence length
    assert observed_number_kmers == 20 # 20 is the hard-coded sequence length
    assert observed_kmer_total_counts == expected_kmer_total_counts
    print_string_as_info('... compute_kmer_total_counts(...) passed test')

def test_compute_singleton_counts(args): 
  print('')
  print('*************************************************')
  print('')
  print_string_as_info('**** testing compute_singleton_counts(...) ***** ')
  print('')
  region = 'chr1:15300-15900'
  with pysam.TabixFile(args.mutations) as mutations, pysam.FastaFile(args.genome) as genome: 
    expected_singleton_counts = {0: [2], 2: [0, 1, 2], 1: [1, 0], 9: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 8: [0, 0, 0, 0, 1, 0, 0, 0, 0], 5: [0, 0, 1, 0, 0, 0], 3: [1, 0, 1, 0]}
    observed_singleton_counts = compute_singleton_counts(region, mutations, genome, args)
    observed_singleton_counts = make_serializable(observed_singleton_counts)
    assert observed_singleton_counts == expected_singleton_counts
    print_string_as_info('... compute_singleton_counts(...) passed test')

if __name__ == '__main__':
  main() 

