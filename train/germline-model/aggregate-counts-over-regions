#!/usr/bin/env python

# pysam API: 
# https://github.com/pysam-developers/pysam/blob/b82cbcae22c088e64fdb58f8acaf1e9773c7b088/pysam/libctabix.pyx
import pysam

import json
import argparse 
import os
import subprocess
import multiprocessing
from tqdm import tqdm

from kmer import initialize_kmer_counts_germline, fetch_kmers
from singleton import Singleton_Counts, make_serializable
from aggregate_counts import aggregate_counts

from colorize import (
  print_json, 
  print_string_as_info, 
  print_string_as_info_dim
)
import color_traceback 
from snvs import fetch_SNVs, reduce_SNVs
from pack_unpack import bed_to_sam_string
from timer import timer 
from windows import create_windows 

# using the "khmer" library might make this function a handful of times faster:
# https://khmer.readthedocs.io/en/v0.6.1-0/ktable.html
def compute_kmer_total_counts(neutral_region, genome, kmer_counts, args):
  for kmer in fetch_kmers(neutral_region, genome, args.kmer_size): 
    kmer_counts[kmer]['count'] += 1
  return kmer_counts

def compute_kmer_SNV_counts(neutral_region, mutations, genome, kmer_counts, args):
  print_string_as_info('Fetching SNVs in region {} and incrementing corresponding kmer counts...'.format(neutral_region))

  SNVs = fetch_SNVs(mutations, genome, neutral_region, args.__dict__)
  SNVs = reduce_SNVs(SNVs)
  for SNV in SNVs: 
    kmer_counts[SNV['kmer']]['ALTStateCounts'][SNV['ALTState']] += 1
  return kmer_counts

def get_hostname_process_cpu(): 
  hostname = os.uname()[1]
  pid = os.getpid()
  cpu = subprocess.run(
    ["ps", "-o", "psr=", "-p", f"{pid}"], 
    capture_output=True
  ).stdout.decode("utf-8").strip()
  return {
    'hostname': hostname,
    'process': pid, 
    'cpu': f'{cpu}/{multiprocessing.cpu_count()}'
  }

def parse_arguments():
  parser = argparse.ArgumentParser(description='')
  parser.add_argument('--kmer-size', type=int, dest='kmer_size', help='')
  parser.add_argument('--genome', type=str, help='')
  parser.add_argument('--number-chromosomes-min', type=int, dest='number_chromosomes_min', help='')
  parser.add_argument('--mutations', type=str, help='')
  parser.add_argument('--window-size', type=int, dest='window_size', help='')
  parser.add_argument('--neutral-regions-filename', type=str, dest='neutral_regions_filename', help='')
  parser.add_argument('--counts-filename', type=str, dest='counts_filename', help='')
  parser.add_argument('--log', type=str, help='')
  return parser.parse_args()

def compute_singleton_counts(neutral_region, mutations, genome, args):
  print_string_as_info('Breaking neutral region into non-overlapping windows, and counting SNVs and singletons in each window...')

  windows = create_windows(
    window_size=args.window_size, 
    window_stride=args.window_size, 
    region=neutral_region, 
    genome=genome, 
    region_contains_windows=True
  )
  print_string_as_info_dim(f'Created {len(windows)} windows within neutral region {neutral_region}')

  singleton_counts = Singleton_Counts()
  for window in windows: 
    SNVs = fetch_SNVs(mutations, genome, window['region'], args.__dict__, args.number_chromosomes_min)
    SNV_count = len(SNVs)
    singleton_count = len([SNV for SNV in SNVs if SNV['number_ALT_chromosomes'] == 1])
    print_string_as_info_dim(f"{singleton_count} of {SNV_count} SNVs in {window['region']} are singletons")
    singleton_counts[SNV_count][singleton_count] += 1

  return singleton_counts

@timer
def count_on_region(neutral_region, args):
  # pysam.FastaFile uses the index produced by "samtools faidx": 
  # https://pysam.readthedocs.io/en/latest/api.html?highlight=fasta#pysam.FastaFile
  with pysam.TabixFile(args.mutations) as mutations, pysam.FastaFile(args.genome) as genome: 
    kmer_counts = initialize_kmer_counts_germline(args) 
    kmer_counts = compute_kmer_total_counts(neutral_region, genome, kmer_counts, args) 
    kmer_counts = compute_kmer_SNV_counts(neutral_region, mutations, genome, kmer_counts, args)

    singleton_counts = compute_singleton_counts(neutral_region, mutations, genome, args)

  return kmer_counts, singleton_counts

# https://realpython.com/introduction-to-python-generators/
def fetch_counts_from_regions(args, progress_bar): 
  print_json({'neutral regions': args.neutral_regions_filename, **get_hostname_process_cpu()})
  with open(args.neutral_regions_filename, 'r') as neutral_regions:
    number_of_neutral_regions = sum(1 for line in neutral_regions)
  with open(args.neutral_regions_filename, 'r') as neutral_regions:
    for neutral_region in tqdm(
      neutral_regions, 
      total=number_of_neutral_regions,
      file=progress_bar, 
      desc='fetch_counts_from_regions'
    ): 
      neutral_region = bed_to_sam_string(neutral_region)
      kmer_counts, singleton_counts = count_on_region(neutral_region, args)      
      yield { 
        'kmerCounts': kmer_counts,
        'singletonCounts': singleton_counts
      }

def aggregate_counts_over_regions(args, progress_bar): 
  # Minimize high-latency IO by ONLY writing counts AFTER aggregation over regions.
  # A similar approach is used by spark to get 100X speed-ups relative to Hadoop:
  # https://www.coursera.org/learn/scala-spark-big-data/lecture/D5o7O/latency

  print_string_as_info('Aggregating kmer and singleton counts over regions...')
  counts = aggregate_counts(fetch_counts_from_regions, args, progress_bar)

  print_string_as_info(f'Aggregated kmer and singleton counts over regions:') 
  print_string_as_info_dim(args.neutral_regions_filename) 
  print_string_as_info('Saved aggregated counts to:')
  print_string_as_info_dim(args.counts_filename)
  
  with open(args.counts_filename, 'w') as fh:
    json.dump({
      'mutations': args.mutations,
      'genome': args.genome,
      'kmerSize': args.kmer_size,
      'numberChromosomesMin': args.number_chromosomes_min,
      'windowSize': args.window_size,
      'kmerCounts': counts['kmerCounts'],
      'singletonCounts': make_serializable(counts['singletonCounts']),
    }, fh, indent=2)

def main(): 
  args = parse_arguments()  
  import sys 
  if args.log == 'stdout': 
    aggregate_counts_over_regions(args, progress_bar=sys.stdout)
  else: 
    with open(args.log, 'w') as progress_bar:  
      # logging tqdm output: 
      # https://github.com/tqdm/tqdm/issues/506#issuecomment-373762049
      # https://github.com/tqdm/tqdm/issues/506#issuecomment-508458426
      aggregate_counts_over_regions(args, progress_bar=progress_bar)

def test_compute_kmer_total_counts(): 
  print('')
  print('*************************************************')
  print('')
  print_string_as_info('**** testing compute_kmer_total_counts(...) ***** ')
  print('')
  args = parse_arguments()  
  genome_filename = '/scratch/ucgd/lustre-work/quinlan/data-shared/constraint-tools/reference/grch38/hg38.analysisSet.fa.gz'
  region = 'chr1:15320-15340'
  with pysam.FastaFile(genome_filename) as genome: 
    kmer_counts = initialize_kmer_counts_germline(args)
    expected_kmer_total_counts = [('AAA', 2), ('AAG', 2), ('AGA', 1), ('AGC', 3), ('AGG', 1), ('CAA', 2), ('CAG', 1), ('GAG', 1), ('GCA', 3), ('GCG', 1), ('GGC', 1)]
    kmer_counts = compute_kmer_total_counts(region, genome, kmer_counts, args)
    observed_kmer_total_counts = [(kmer, data['count']) for kmer, data in kmer_counts.items() if data['count'] > 0]
    observed_number_kmers = sum([count for _, count in observed_kmer_total_counts])
    assert observed_number_kmers == 20 - (args.kmer_size - 1) # 20 is the hard-coded sequence length
    assert observed_kmer_total_counts == expected_kmer_total_counts
    print_string_as_info('... compute_kmer_total_counts(...) passed test')

if __name__ == '__main__':
  main() 
  test_compute_kmer_total_counts()  

